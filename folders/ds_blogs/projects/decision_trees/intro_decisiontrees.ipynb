{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
       "    return false;\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border-top: 5px solid black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:blue;\">Decision Trees, Machine learning, Classification</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border-top: 5px solid black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" style=\"font-size:20px; border:1px solid black; padding:10px\">\n",
    "<center><h1>Post Goals:</h1></center>\n",
    "    <hr style=\"border-top: 2px dashed black;\">\n",
    "    <ol>\n",
    "        <li>Provide a brief introduction to Decision Tree.</li><br>\n",
    "        <li>Demonstrate how to train and visualize a decision tree using python.</li><br>\n",
    "    </ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border-top: 5px solid black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"objective\" class=\"alert alert-block alert-warning\" style=\"font-size:16px; border:1px solid black; padding:10px\"><center><h1><br><font color=\"blue\">Brief Introduction to Decision Trees Classification.</font></h1></center><br>\n",
    "</div>\n",
    "<div style=\"font-size:16px; border:1px solid black; padding:10px\">\n",
    "    <ul><strong><u>Basic Description of Decision Trees</u></strong>\n",
    "        <li>Is an algorithm that is part of the family of supervised machine learning models.</li><br> \n",
    "        <li>Decision trees are versatile machine learning algorithms that can carry out classification and regression.</li><br>\n",
    "        <li>This post will focus on classification.</li><br>\n",
    "        <li>Decision trees can fit complex datasets, and are a fundamental component of the Random Forest machine learning algorithm.</li><br> \n",
    "        <li>Decision trees are also a non-parametric supervised learning method, and the goal is to create a model that predicts a target variable value by learning simple decision rules that the algorithm infers from the features of the data.</li><br>\n",
    "        <li>Decision trees essentially allow us to run a series of <strong>if/elif/else</strong> tests on a data point, and each node of this tree represents a condition of an attribute to test.</li><br>        \n",
    "        <li>An observation travels through each stage to reach a leaf node.</li><br> \n",
    "        <li>The leaf contains the final proposed classification.</li><br>\n",
    "        <li>The image below is a good illustration of these concepts.</li><br>\n",
    "        <li style=\"list-style-type: none;\"><img src=\"projects/decision_trees/static/images/image1.png\"></li><br>\n",
    "        <li>In this example we have a dataset with four predictor features and one target feature.</li><br>\n",
    "        <li>The predictor and target features are nominal, with the target and final classification being binomial (play golf, do not play golf).</li><br>        \n",
    "    </ul>\n",
    "<hr style=\"border-top: 2px solid black;\">    \n",
    "    <ul><strong><u>Advantages</u></strong>\n",
    "        <li>Decision trees are easy to interpret and understand.</li><br>        \n",
    "        <li>Minimal data prep is needed, and decision trees do not need data to be normalization, dummy variables created, or blank values removed.</li><br>        \n",
    "        <li>This method can handle both numeric and categorical data, although Sci-kit learn has not currently support categorical variables.</li><br> \n",
    "        <li>Decision trees can handle multi-output problems, and uses a white box model, where the explanation can be explained by boolean logic.</li><br>\n",
    "        <li>Decision trees perform well even if its assumptions are violated.</li><br>\n",
    "    </ul> \n",
    "    <hr style=\"border-top: 2px solid black;\">\n",
    "    <ul><strong><u>Disadvantages</u></strong>\n",
    "        <li>Prone to over-complex trees that do not generalize well to new data.</li><br> \n",
    "        <li>To prevent model overfitting requires minimizing the number of samples required at a leaf node, or setting the maximum depth of the tree.</li><br>        \n",
    "        <li>Small variations in the data can result in unstable trees where a new tree can be generated. Ensembles are generally used to mitigate this problem.</li><br> \n",
    "        <li>Decision trees are not good at extrapolation.</li><br>\n",
    "        <li>Decision tree learners can result in biased trees if some classes dominate, to avoid this the dataset needs to be balanced prior to fitting a decision tree model.</li><br>     \n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border-top: 5px solid black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\" style=\"font-size:16px; border:1px solid black; padding:10px;\"><center><h1><br><font color=\"blue\">Demonstrate how to train and visualize a decision tree using python.</font></h1></center><br>\n",
    "    </div>\n",
    "<div style=\"font-size:16px; border:1px solid black; padding:10px\">\n",
    "    <ol>\n",
    "        <li><a href=\"#objective1\">Import Dependencies.</a></li><br>\n",
    "        <li><a href=\"#objective2\">Load Data.</a></li><br>\n",
    "        <li><a href=\"#objective3\">Make Predictions.</a></li><br>\n",
    "        <li><a href=\"#objective4\">Estimate Class Probabilities.</a></li><br>\n",
    "<!--         <li><a href=\"#objective5\">.</a></li><br> -->\n",
    "    </ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border-top: 5px solid black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Import Dependencies</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "# Data set\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Decision tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size:16px; border:1px solid black; padding:10px\">\n",
    "<center><h1><br><font color=\"blue\">Packages Explained</font></h1></center><br>\n",
    "<hr style=\"border-top:2px solid black;\">\n",
    "    <ul>\n",
    "        <li><strong><code><a href=\"https://docs.python.org/3/library/sys.html\">sys</a></code> module provides access to variables used or maintained by the interpreter and to functions that interact strongly with the interpreter.</strong></li><br>\n",
    "        <li><strong><code><a href=\"https://scikit-learn.org/stable/\">scikit-learn</a></code> module provides Simple and efficient tools for predictive data analysis. Package will be used to invoke decision tree classifier. The <code>assert sklearn.__version__ >= \"0.20\"</code> ensures we use a more recent version.</strong></li><br>\n",
    "        <li><strong><code><a href=\"https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html\">from sklearn.datasets import load_iris</a></code> loads a standard machine learning dataset that is used in classification problems.</strong></li><br>\n",
    "        <li><strong><code><a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\">from sklearn.tree import DecisionTreeClassifier</a></code> imports the decision tree classifier.</strong></li><br>\n",
    "        <li><strong><code><a href=\"https://numpy.org/\">numpy</a></code> is a fundamental package for scientific computing with Python.</strong></li><br>\n",
    "<!--         <li><strong><code><a href=\"https://docs.python.org/3/library/os.html\">os</a></code> module provides a portable way to use operating system dependent functionality.</strong></li> -->\n",
    "        <li><strong><code><a href=\"https://matplotlib.org/\">matplotlib</a></code> is a comprehensive library for creating static, animated, and interactive visualizations in python.</strong></li><br>\n",
    "        <li><strong><code><a href=\"https://matplotlib.org/\">import matplotlib as mpl</a></code> used to customize global plot parameters.</strong></li><br>\n",
    "        <li><strong><code><a href=\"https://matplotlib.org/\">import matplotlib.pyplot as plt</a></code> used to import pyplot as plt and used in the code to generate graphics.</strong></li><br>     \n",
    "    </ul>\n",
    "</div>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border-top:4px solid black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size:16px; border:1px solid black; padding:10px\">\n",
    "<center><h1><br><font color=\"blue\">Load Data</font></h1></center><br>\n",
    "<hr style=\"border-top:2px solid black;\">\n",
    "    <ul>\n",
    "        <li><strong><code><a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html\">Sklearn Iris Dataset</a></code></strong> is a toy dataset commonly used in machine learning models for pattern recognition literature and was created by R.A. Fisher, July 1988.</li><br>\n",
    "        <li>Fisher, R.A. “The use of multiple measurements in taxonomic problems” Annual Eugenics, 7, Part II, 179-188 (1936); also in “Contributions to Mathematical Statistics” (John Wiley, NY, 1950).</li>        \n",
    "        <li>The data consists of 150 instances, three classes, and four attributes. Each class refers to a type of iris plant, where one class linearly separable from the other two.</li><br>\n",
    "        <li>The iris dataset is used in this example to classify data into either class.</li><br>\n",
    "    </ul>\n",
    "    <hr style=\"border-top:2px solid black;\">\n",
    "    <ul><strong>Code:</strong>\n",
    "        <li><strong><code>iris = load_iris()</code></strong>: Loads and return the iris dataset (classification). Returns dictionary-like object with the following attributes:\n",
    "            <ul><strong><u>Attributes</u></strong>\n",
    "                <li>Data {ndarray, dataframe} shape (150, 4). If <code>as_frame=True</code> data will be a pandas series.</li><br>\n",
    "                <li>target: {ndarray, Series} of shape (150,). The classification target. If <code>as_frame=True</code>, target will be a pandas Series.</li><br>\n",
    "                <li>feature_names: list. The names of the datatset columns.</li><br>\n",
    "                <li>target_names: list. The names of target classes.</li><br>\n",
    "                <li>frame: DataFrame of shape (150, 5). Only present when as_frame=True. DataFrame with data and target.</li><br>\n",
    "                <li>DESCR: str. The full description of the dataset.</li><br>\n",
    "                <li>filename: str. The path to the location of the data.</li><br>\n",
    "                <li>(data, target). tuple if return_X_y is True</li><br>                \n",
    "            </ul>\n",
    "        </li><br>\n",
    "        <li><strong><code>X = iris.data[:, 2:]</code></strong>: saves the data without the target into the X variable. X-shape (150,2). The shape of the data is important for the training to work.</li><br>\n",
    "        <li><strong><code></code>y = iris.target</strong>: saves the target data into the y variable with the shape (150).</li><br>      \n",
    "    </ul>\n",
    "</div>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris() # loads data\n",
    "X = iris.data[:, 2:] # petal length and width\n",
    "y = iris.target # saves the target feature (iris type) to y variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border-top:4px solid black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Instantiate and Train DecisionTreeClassifier Model</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(max_depth=2, random_state=42)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_clf = DecisionTreeClassifier(max_depth=2, random_state=42) #instaiates the model\n",
    "tree_clf.fit(X, y) # trains model on data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size:16px; border:1px solid black; padding:10px\">\n",
    "<center><h1><br><font color=\"blue\">Code Explained</font></h1></center><br>\n",
    "<hr style=\"border-top:2px solid black;\">\n",
    "    <ul><strong><u>Code Block Explained:</u></strong>\n",
    "        <li><strong><code>DecisionTreeClassifier(max_depth=2, random_state=42)</code></strong>: Instantiates the DecisionTreeClassifier with a max depth of the tree to 2 levels and the random state set to 42, which controls the randomness of the estimator. 42 is a common int used and is a tribute to the <em>tribute to the \"Hitch-hiker's Guide\" books by Douglas Adams</em>.</li><br>\n",
    "        <li><strong><code></code>tree_clf.fit(X, y)</strong>: fits the iris data to the DecisionTreeClassifier. The <code>X</code> are the data features and the <code>y</code> is the target feature, which is what we hope to classify at the end of the decision tree.</li><br> \n",
    "    </ul>\n",
    "    <hr style=\"border-top:2px solid black;\">\n",
    "    <ul><strong><u>Background Information:</u></strong>\n",
    "        <li><strong><code><a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\">Decision tree classifier</a></code></strong> is a non-parametric supervised learning method used in this example for classification, but can also be used for regression.</li><br>\n",
    "        <li>The goal of the model is to create a model that predicts the value of the target variable by learning patterns from the data following simple decision rules inferred from the features of the data.</li>        \n",
    "        <li>A tree diagram is a piecewise constant approximation.</li><br>\n",
    "    </ul>\n",
    "    <hr style=\"border-top:2px solid black;\">\n",
    "    <ul><u><strong>DecisionTreeClassifier Explained:</strong></u>\n",
    "        <li><strong><code>class sklearn.tree.DecisionTreeClassifier(*, criterion='gini', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, class_weight=None, ccp_alpha=0.0)</code></strong>: \n",
    "            <ul><strong><u>Some Key Parameters</u></strong>\n",
    "                <li><strong><code>criterion</code> {“gini”, “entropy”} default=”gini”</strong>: The function to measure the quality of a split. Gini looks at impurity \\(G_{i} = 1 - \\Sigma_{k=1}^{n}p_{i,k}^{2} \\) and entropy at information gain \\(H_{i} = -\\Sigma^{n}_{k=1_{p_{i,k} \\neq 0}}\\log_{2}(p_{i,k}) \\). </li><br>\n",
    "                <li><strong><code>splitter</code> {“best”, “random”}, default=”best”</strong>: The strategy used to choose the split at each node. Supported strategies are “best” to choose the best split and “random” to choose the best random split.</li><br>\n",
    "                <li><strong><code>max_depth</code> int, default=None</strong>: The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.</li><br>\n",
    "                <li><strong><code>min_samples_split</code> int or float, default=2</strong>: The minimum number of samples required to split an internal node: If int, then consider <code>min_samples_split</code> as the minimum number. If float, then <code>min_samples_split</code> is a fraction and <code>ceil(min_samples_split * n_samples)</code> are the minimum number of samples for each split.</li><br>\n",
    "                <li><strong><code>min_samples_leaf</code> int or float, default=1</strong>: The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least min_samples_leaf training samples in each of the left and right branches. This may have the effect of smoothing the model, especially in regression.</li><br>\n",
    "                <li><strong><code>min_weight_fraction_leaf</code> float, default=0.0</strong>: The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Samples have equal weight when sample_weight is not provided..</li><br>\n",
    "                <li><strong><code>max_features</code> int, float or {“auto”, “sqrt”, “log2”}, default=None</strong>: The number of features to consider when looking for the best split. If int, then consider max_features features at each split. If float, then max_features is a fraction and int(max_features * n_features) features are considered at each split. If “auto”, then max_features=sqrt(n_features). If “sqrt”, then max_features=sqrt(n_features). If “log2”, then max_features=log2(n_features). If None, then max_features=n_features.</li><br>\n",
    "                <li><strong><code>random_state</code> int, RandomState instance or None, default=None</strong>: Controls the randomness of the estimator. The features are always randomly permuted at each split, even if splitter is set to \"best\". When max_features $<$ n_features, the algorithm will select max_features at random at each split before finding the best split among them. But the best found split may vary across different runs, even if max_features=n_features. That is the case, if the improvement of the criterion is identical for several splits and one split has to be selected at random. To obtain a deterministic behaviour during fitting, random_state has to be fixed to an integer.</li><br>\n",
    "                <li><strong><code>max_leaf_nodes</code></strong>: int, default=None. Grow a tree with max_leaf_nodes in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes.</li><br>\n",
    "                <li><strong><code>class_weight</code></strong>: dict, list of dict or “balanced”, default=None. Weights associated with classes in the form {class_label: weight}. If None, all classes are supposed to have weight one. For multi-output problems, a list of dicts can be provided in the same order as the columns of y. The “balanced” mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as n_samples / (n_classes * np.bincount(y))</li><br>\n",
    "                <li><strong><code>min_impurity_decrease</code> float, default=0.0</strong>: A node will be split if this split induces a decrease of the impurity greater than or equal to this value.</li><br> \n",
    "            </ul>\n",
    "        </li><br>\n",
    "    </ul>\n",
    "    <hr style=\"border-top:2px solid black;\">\n",
    "    <ul><strong><u>Key Attributes</u></strong>\n",
    "        <li><strong><code>classes_</code> ndarray of shape (n_classes,) or list of ndarray</strong>: The classes labels (single output problem), or a list of arrays of class labels (multi-output problem).</li><br>\n",
    "        <li><strong><code>feature_importances_</code> ndarray of shape (n_features,)</strong>: Return the feature importances.</li><br>\n",
    "        <li><strong><code>max_features_</code> int</strong>: The inferred value of max_features.</li><br>\n",
    "        <li><strong><code>n_classes_</code> int or list of int</strong>: The number of classes (for single output problems), or a list containing the number of classes for each output (for multi-output problems).</li><br> \n",
    "        <li><strong><code>n_features_</code> int</strong>: The number of features when fit is performed.</li><br>\n",
    "        <li><strong><code>n_outputs_</code> int</strong>: The number of outputs when fit is performed.</li><br>  \n",
    "        <li><strong><code>tree_</code> Tree instance</strong>: The underlying Tree object.</li><br>\n",
    "<!--         <li><strong><code></code></strong>: .</li><br>         -->\n",
    "    </ul>\n",
    "</div>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border-top:4px solid black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Visualize Decision Tree using Graphviz and Skelearn Tree</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.47.3 (20210619.1520)\n",
       " -->\n",
       "<!-- Title: Tree Pages: 1 -->\n",
       "<svg width=\"360pt\" height=\"314pt\"\n",
       " viewBox=\"0.00 0.00 360.00 314.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 310)\">\n",
       "<title>Tree</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-310 356,-310 356,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>0</title>\n",
       "<path fill=\"#ffffff\" stroke=\"black\" d=\"M215,-306C215,-306 67,-306 67,-306 61,-306 55,-300 55,-294 55,-294 55,-235 55,-235 55,-229 61,-223 67,-223 67,-223 215,-223 215,-223 221,-223 227,-229 227,-235 227,-235 227,-294 227,-294 227,-300 221,-306 215,-306\"/>\n",
       "<text text-anchor=\"middle\" x=\"141\" y=\"-290.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal length (cm) &lt;= 2.45</text>\n",
       "<text text-anchor=\"middle\" x=\"141\" y=\"-275.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.667</text>\n",
       "<text text-anchor=\"middle\" x=\"141\" y=\"-260.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 150</text>\n",
       "<text text-anchor=\"middle\" x=\"141\" y=\"-245.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [50, 50, 50]</text>\n",
       "<text text-anchor=\"middle\" x=\"141\" y=\"-230.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = setosa</text>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>1</title>\n",
       "<path fill=\"#e58139\" stroke=\"black\" d=\"M108,-179.5C108,-179.5 12,-179.5 12,-179.5 6,-179.5 0,-173.5 0,-167.5 0,-167.5 0,-123.5 0,-123.5 0,-117.5 6,-111.5 12,-111.5 12,-111.5 108,-111.5 108,-111.5 114,-111.5 120,-117.5 120,-123.5 120,-123.5 120,-167.5 120,-167.5 120,-173.5 114,-179.5 108,-179.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"60\" y=\"-164.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n",
       "<text text-anchor=\"middle\" x=\"60\" y=\"-149.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 50</text>\n",
       "<text text-anchor=\"middle\" x=\"60\" y=\"-134.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [50, 0, 0]</text>\n",
       "<text text-anchor=\"middle\" x=\"60\" y=\"-119.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = setosa</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;1 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>0&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M112.9,-222.91C105.11,-211.65 96.64,-199.42 88.8,-188.11\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"91.53,-185.9 82.96,-179.67 85.78,-189.88 91.53,-185.9\"/>\n",
       "<text text-anchor=\"middle\" x=\"78.49\" y=\"-200.56\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">True</text>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>2</title>\n",
       "<path fill=\"#ffffff\" stroke=\"black\" d=\"M293.5,-187C293.5,-187 150.5,-187 150.5,-187 144.5,-187 138.5,-181 138.5,-175 138.5,-175 138.5,-116 138.5,-116 138.5,-110 144.5,-104 150.5,-104 150.5,-104 293.5,-104 293.5,-104 299.5,-104 305.5,-110 305.5,-116 305.5,-116 305.5,-175 305.5,-175 305.5,-181 299.5,-187 293.5,-187\"/>\n",
       "<text text-anchor=\"middle\" x=\"222\" y=\"-171.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal width (cm) &lt;= 1.75</text>\n",
       "<text text-anchor=\"middle\" x=\"222\" y=\"-156.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.5</text>\n",
       "<text text-anchor=\"middle\" x=\"222\" y=\"-141.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 100</text>\n",
       "<text text-anchor=\"middle\" x=\"222\" y=\"-126.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 50, 50]</text>\n",
       "<text text-anchor=\"middle\" x=\"222\" y=\"-111.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;2 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>0&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M169.1,-222.91C175.26,-214.01 181.84,-204.51 188.19,-195.33\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"191.13,-197.24 193.95,-187.02 185.38,-193.25 191.13,-197.24\"/>\n",
       "<text text-anchor=\"middle\" x=\"198.42\" y=\"-207.92\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">False</text>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>3</title>\n",
       "<path fill=\"#4de88e\" stroke=\"black\" d=\"M202,-68C202,-68 102,-68 102,-68 96,-68 90,-62 90,-56 90,-56 90,-12 90,-12 90,-6 96,0 102,0 102,0 202,0 202,0 208,0 214,-6 214,-12 214,-12 214,-56 214,-56 214,-62 208,-68 202,-68\"/>\n",
       "<text text-anchor=\"middle\" x=\"152\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.168</text>\n",
       "<text text-anchor=\"middle\" x=\"152\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 54</text>\n",
       "<text text-anchor=\"middle\" x=\"152\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 49, 5]</text>\n",
       "<text text-anchor=\"middle\" x=\"152\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;3 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>2&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M195.93,-103.73C190.34,-94.97 184.41,-85.7 178.79,-76.91\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"181.62,-74.84 173.29,-68.3 175.72,-78.61 181.62,-74.84\"/>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>4</title>\n",
       "<path fill=\"#843de6\" stroke=\"black\" d=\"M340,-68C340,-68 244,-68 244,-68 238,-68 232,-62 232,-56 232,-56 232,-12 232,-12 232,-6 238,0 244,0 244,0 340,0 340,0 346,0 352,-6 352,-12 352,-12 352,-56 352,-56 352,-62 346,-68 340,-68\"/>\n",
       "<text text-anchor=\"middle\" x=\"292\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.043</text>\n",
       "<text text-anchor=\"middle\" x=\"292\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 46</text>\n",
       "<text text-anchor=\"middle\" x=\"292\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 1, 45]</text>\n",
       "<text text-anchor=\"middle\" x=\"292\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;4 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>2&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M248.07,-103.73C253.66,-94.97 259.59,-85.7 265.21,-76.91\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"268.28,-78.61 270.71,-68.3 262.38,-74.84 268.28,-78.61\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.files.Source at 0x112069eb0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from graphviz import Source\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "export_graphviz(\n",
    "        tree_clf,\n",
    "        out_file=\"static/images/iris_tree.dot\",\n",
    "        feature_names=iris.feature_names[2:],\n",
    "        class_names=iris.target_names,\n",
    "        rounded=True,\n",
    "        filled=True\n",
    "    )\n",
    "\n",
    "Source.from_file(\"static/images/iris_tree.dot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size:16px; border:1px solid black; padding:10px\">\n",
    "<center><h1><br><font color=\"blue\">Code Explained</font></h1></center>\n",
    "    <ul><strong><u>Imported Packages Explained:</u></strong>\n",
    "        <li><strong><code><a href=\"https://pypi.org/project/graphviz/#description\">graphviz</a></code></strong> package facilitates the creation and rendering of graph descriptions in the DOT language of the graphviz graph drawing software. The <code>import source</code> code is used for Verbatim DOT source code string to be rendered by Graphviz.</li><br>\n",
    "    <li>We use graphviz to aid in the rendering of our decision tree.</li>\n",
    "    <li><strong><code><a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.tree.export_graphviz.html\">sklearn.tree.export_graphviz</a></code></strong> Export a decision tree in DOT format.</li><br>\n",
    "    <li>The function code <code>sklearn.tree.export_graphviz(decision_tree, out_file=None, *, max_depth=None, feature_names=None, class_names=None, label='all', filled=False, leaves_parallel=False, impurity=True, node_ids=False, proportion=False, rotate=False, rounded=False, special_characters=False, precision=3)</code> code is used for Verbatim DOT source code string to be rendered by Graphviz.</li><br>\n",
    "    </ul>\n",
    "    <hr style=\"border-top:2px solid black;\">\n",
    "    <ul><strong><u>Code Block Explained:</u></strong>\n",
    "        <li><strong><code>tree_clf</code> parameter</strong>: The first parameter is the decision tree that we instantiated earlier and trained with our iris dataset. This will be exported using GraphViz.</li><br>\n",
    "        <li><strong><code>out_file</code> parameter</strong>: Handle or name of the output file. If None, the result is returned as a string. Here we are saving the file in a nested subfolder.</li><br>\n",
    "        <li><strong><code>feature_names</code> parameter</strong>: Names of each of the features. This is extracted using the <code>feature_names</code> attribute for the iris dataset we loaded from sklearn.</li><br>\n",
    "        <li><strong><code>class_names</code> parameter</strong>: This is the target labels that are obtained from the iris dataset using the <code>target_names</code> attribute.</li><br>\n",
    "        <li><strong><code>rounded</code> parameter</strong>: When set to True, draw node boxes with rounded corners and use Helvetica fonts instead of Times-Roman. This is used for styling.</li><br>\n",
    "        <li><strong><code>filled</code> parameter</strong>: When set to True, paint nodes to indicate majority class for classification, extremity of values for regression, or purity of node for multi-output.</li><br>\n",
    "        <li><strong><code>Source.from_file(\"static/images/iris_tree.dot\")</code></strong>: uses the graphviz <code>from_file</code> class method to return an instance with the source string read from the given file and displays it in the jupyter notebook.</li><br>\n",
    "<!--         <li><strong><code></code></strong>: .</li><br>         -->\n",
    "    </ul>\n",
    "    <hr style=\"border-top:2px solid black;\">\n",
    "    <ul><strong><u>Decision Tree Prediction Explained:</u></strong>\n",
    "        <li><strong><code>Number of levels</code></strong>: in the example above we see the root node and a max depth of two levels, which we expect since we set the <code>max_depth=2</code> level to 2.</li><br>\n",
    "        <li><strong><code>root node</code></strong>: This is where the depth = 0 and is at the very top. The model determined that the first question to ask is whether the flower's petal length is smaller than 2.45 cm.  If it is, then you move down to the left node, and if it is not true, then it moves to the right node.</li><br>\n",
    "        <li><strong><code>Depth 1, Left node</code></strong>: This node contains instances of the data that have petal lengths (cm) less than 2.45. The model classifies these data as class setosa. There are a few other descriptions in this node.  The gini is a measure of how pure the data are in this node. It has a 0.0 gini score which means it is pure. Purity means that all instances belong to the same class. The node also provides the number of samples, which is 50, and the list of values <code>[50,0,0]</code> correspond to the instances of data in that node that fall under each class. We have three classes in this dataset, and since this is a \"pure\" node, we only have values in the first index of the list, which is 50 data of setosa class.</li><br>\n",
    "        <li><strong><code>Depth 1, right node</code></strong>: The model groups all data that have greater than 1.75 cm petal width.  The gini score is 0.5, which means the data are split between two classes  The sample size is 100, since we have two classes of data here, and each class has 50 instances.  The value is <code>[0, 50, 50]</code> which is consistent with the presence of no setosa, but 50 instances of the other two classes. Finally the class here is listed as versicolor, but that is not true for all data. This means we need to go down another level to separate all the instances in this node into their respective classes.</li><br>\n",
    "        <li><strong><code>Depth 2, Left node</code></strong>: This green node contains the class of versicolor. it has a gini purity score of 0.168, which means that there are some data from the virginica class that have been grouped into this node. This is because some virginica flowers had petal width less than 1.75.  The value list provides specific numbers for these instances <code>[0, 49, 5]</code>. This means we have 49 data that are correct versicolor, and 5 instances that are virginica but are less than 1.75 cm in petal width.</li><br>\n",
    "        <li><strong><code>Depth 2, Right node</code></strong>: This violet node contains the class of virginica. It has a gini purity score of 0.043, and the values list of <code>[0, 1, 45]</code> provides a breakdown of each class instance in this node. This list confirms no data from the setosa class, 1 data from the versicolor class, and 45 from the virginica class.  The 1 versicolor class data point had a petal width greater than 1.75 cm.</li><br>\n",
    "        <li><strong><code>Final Comments</code></strong>: Since this model only had three classes with a small sample size, we were able to use the decision tree classifier and graphviz to prepare a decision tree. This decision tree aids in prediction by setting feature conditions to help sort the data into specific classes. The overall gini scores also provide additional evidence for the model performance and low misclassification rates. The only misclassifications occured between versicolor and verginica as their petal widths have some overlap.</li><br>\n",
    "<!--         <li><strong><code></code></strong>: .</li><br>         -->\n",
    "    </ul>\n",
    "</div>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border-top:4px solid black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size:16px; border:1px solid black; padding:10px\">\n",
    "<center><h1>Estimating Class Probabilities</h1></center>\n",
    "        <hr style=\"border-top:2px solid black;\">\n",
    "<ul><strong><u></u></strong>\n",
    "    <li>A decision tree also provides the probability that an instance belongs to given class <code>k</code></li><br>\n",
    "        <li>You can obtain the predicted probability for a given data point by using the <code><a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html?highlight=predict_proba#sklearn.tree.DecisionTreeClassifier.predict_proba\">predict_proba()</a></code> class.</li><br>\n",
    "    <li><code>predict_proba(X, check_input=True)</code>: the parameters for this class include X and check_input.</li><br>\n",
    "        <li><strong><code>X</code> {array-like, sparse matrix}:</strong> of shape (n_samples, n_features): The input samples. Internally, it will be converted to dtype=np.float32 and if a sparse matrix is provided to a sparse csr_matrix.</li><br>\n",
    "        <li><strong><code>check_input</code> bool, default=True:</strong> Allow to bypass several input checking. Don’t use this parameter unless you know what you do.</li><br>\n",
    "        <li><strong><code>Returns</code> probandarray of shape (n_samples, n_classes) or list of n_outputs such arrays if n_outputs > 1:</strong> The class probabilities of the input samples. The order of the classes corresponds to that in the attribute.</li><br>\n",
    "        <li>The examples below will use a petal length and width measurements for a hypothetical flowers, and the <code>predict_proba()</code> and <code>predict()</code> classes will be used to determine the probability and predicted class.</li><br>    \n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.90740741, 0.09259259]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_clf.predict_proba([[5, 1.6]]) # flower with 5 cm petal lenth and #1.5 petal width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This example returns the probabilities for setosa, versicolor and virginica\n",
    "# we see that the probability for setosa is 0, the probability for versicolor is >0.91 \n",
    "# and the probability for virginica is 0.09.  Thus this data is most likely versicolor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_clf.predict([[5, 1.6]]) # this will provide a prediction for which class this data would fall under"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one is for versicolor, so this confirms what we expected from the predict_proba step above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border-top: 5px solid black;\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
