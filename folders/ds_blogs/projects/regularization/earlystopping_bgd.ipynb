{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
       "    return false;\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center style=\"font-size:18px;\"><h1><strong>Introduction:</strong> <font color=\"red\"><strong><em>Early Stopping</em></strong></font> in regularized iterative maching learning algorithms using Gradient Descent.</h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size:16px; border:1px solid black; padding:10px\">\n",
    "    <center><h1>Machine Learning Training Issues.</h1></center>\n",
    "<ul>\n",
    "    <li>Not enough training of your model will yield underfitting of both training and learning data sets.</li><br>    \n",
    "    <li>Too much training will have the opposit affect, overfit the training dataset and this will result in poor performance on the test set.</li><br>\n",
    "    <li>A compromise is to stop the training early, and to do so when the performance on a validation dataset starts to degrade.</li><br>\n",
    "    <li>In other words, early stopping is a method to stop training you rmodel when the performance of your model on the validation data no longer does well.</li><br>\n",
    "    <li>A common metric used to evaluate the performance of a model is the Root Mean Square Error (RMSE).</li><br>\n",
    "    <li>This approach is common used in complex machine learning models, such as Neural Networks.</li><br>\n",
    "    <li>The benefits of early stopping is that you can prevent overfitting your model, and improves your models ability to generalize to new data.</li><br>   \n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size:16px; border:1px solid black; padding:10px\">\n",
    "    <center><h1>Principles of Early Stopping.</h1></center>\n",
    "<ul>\n",
    "    <li>Early stopping requires that a model is training multiple times using different set of parameter values, and then select the training model that had the best performance on the validation set (lowest RMSE).</li><br>    \n",
    "    <li>An <strong>Epoch</strong> is a training model with a unique set of parameter values.</li><br>\n",
    "    <li>Early stopping is like using a for-loop over a number of epochs, and each epoch iterates over each batch of samples and trains a model.</li><br>    \n",
    "    <li>The number of epochs that is used during early stop if often large, allowing the learning algorithm to run until the error from the model has been sufficiently minimized.</li><br>\n",
    "    <li>Learning plots are often used to monitor this process, where the number of epochs are displayed on the x-axis as time and the error (example RMSE) on the y-axis. </li><br>\n",
    "    <li>Two lines are plotted, a training set and a validation set.</li><br>\n",
    "    <li>The training set curve shows you how well the model fits the training set, in terms of error, as the number of epochs increases.</li><br>\n",
    "    <li>The validation set demonstrates how well the model generalizes to new data as the number of epochs increases.</li><br> \n",
    "    <li>These plots help diagnose whether the model has over learned, under learned, or is suitably fit to the training dataset.</li><br>\n",
    "    <li>This process is illustrated in below.</li><br>    \n",
    "    <li>Early stopping can be described in three steps:\n",
    "        <ol>\n",
    "            <li>Monitoring model performance</li>\n",
    "            <li>Trigger to stop training</li>\n",
    "            <li>Model Selection</li>\n",
    "        </ol>\n",
    "    </li><br>\n",
    "    <li>The downside of early stopping is that it is computationally inefficient and time-consuming, especially for large models trained on large datasets, because it requires multiple models to be trained and discarded.</li><br>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border-top: 3px solid Black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size:16px; border:1px solid black; padding:10px\">\n",
    "    <center><h1>Post Goal</h1>\n",
    "     </center><br>\n",
    "    <center style=\"font-size:20px;\">Demonstrate how to carry out early stopping techniques using Batch Gradient Descent as an example.</center>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border-top: 5px solid RED;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Import Dependencies</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# Scikit-Learn â‰¥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "# machine learning\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.base import clone\n",
    "\n",
    "# Math and dataframe modules\n",
    "import numpy as np\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# Data Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Formats plots and uses seaborn theme\n",
    "plt.style.use('seaborn')\n",
    "plt.rc('font', size=14)\n",
    "plt.rc('figure', titlesize=18)\n",
    "plt.rc('axes', labelsize=15)\n",
    "plt.rc('axes', titlesize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border-top: 3px solid Black;\">\n",
    "<h1>Data</h1>\n",
    "<ul>\n",
    "    <li>Randomly generated using numpy</li>\n",
    "    <li><code>m</code> is the size of the array of data</li>\n",
    "    <li><code>np.random.rand(m, 1)</code> is a 2D array with values between 0 - 1</li>\n",
    "    <li><code><strong>X</strong> = 6 * np.random.rand(m, 1) - 3</code> each value between 0 - 1 is multiplied to 6, and then subtracted by 3, which then makes the domain for X between 0 - 3</li>\n",
    "    <li><strong>y</strong> is a polynomial function where:\n",
    "        <ul>\n",
    "            <li><code>np.random.rand(m, 1)</code> is a 2d array with values between 0 - 1</li>\n",
    "            <li><code>X**2</code> each value in the arrray is added to the square of the X value (0 - 3^2) that is also divided by half</li>\n",
    "            <li>This value is then added to the X value (0 - 3)</li>\n",
    "            <li>A constant of 2 is added.</li>\n",
    "        </ul>   \n",
    "</ul>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Generate data</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "m = 100 # size of array\n",
    "X = 6 * np.random.rand(m, 1) - 3\n",
    "y = 2 + X + 0.5 * X**2 + np.random.randn(m, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Split data into Equal Sized Groups of Training and Test Set</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X[:50], y[:50].ravel(), test_size=0.5, random_state=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Carry out Early Stopping Methods using Polynomial Regression and Gradient Descent</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_scaler = Pipeline([\n",
    "        (\"poly_features\", PolynomialFeatures(degree=90, include_bias=False)),\n",
    "        (\"std_scaler\", StandardScaler())\n",
    "    ])\n",
    "\n",
    "# transform data\n",
    "X_train_poly_scaled = poly_scaler.fit_transform(X_train)\n",
    "X_val_poly_scaled = poly_scaler.transform(X_val)\n",
    "\n",
    "# instantiate gradient descent\n",
    "sgd_reg = SGDRegressor(max_iter=1, tol=-np.infty, warm_start=True,\n",
    "                       penalty=None, learning_rate=\"constant\", eta0=0.0005, random_state=42)\n",
    "\n",
    "minimum_val_error = float(\"inf\")\n",
    "best_epoch = None\n",
    "best_model = None\n",
    "\n",
    "# loop through a range of epoch's\n",
    "for epoch in range(1000):\n",
    "    sgd_reg.fit(X_train_poly_scaled, y_train)  # continues where it left off because warm_start=True\n",
    "    y_val_predict = sgd_reg.predict(X_val_poly_scaled)\n",
    "    val_error = mean_squared_error(y_val, y_val_predict)\n",
    "    \n",
    "    # here we check for the best error\n",
    "    if val_error < minimum_val_error:\n",
    "        minimum_val_error = val_error\n",
    "        best_epoch = epoch\n",
    "        best_model = clone(sgd_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size:16px; border:1px solid black; padding:10px\">\n",
    "    <center><strong>Comments:</strong></center>\n",
    "<ul> \n",
    "    <li>When the fit method is called on the SGDRegressor with <code>warm_start=True</code>, then the method continues to train where it left off instead of restarting from scratch.</li><br>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Plot Learning Curves to Diagnose Early Stopping</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_reg = SGDRegressor(max_iter=1, tol=-np.infty, warm_start=True,\n",
    "                       penalty=None, learning_rate=\"constant\", eta0=0.0005, random_state=42)\n",
    "\n",
    "n_epochs = 500 # number of epochs\n",
    "train_errors, val_errors = [], [] # instantiate two empty arrays\n",
    "for epoch in range(n_epochs): # early stopping for loop to train up to 500 epochs\n",
    "    sgd_reg.fit(X_train_poly_scaled, y_train) # fit gradient descent model with x trained poly scaled and y train\n",
    "    y_train_predict = sgd_reg.predict(X_train_poly_scaled) # predict y train value\n",
    "    y_val_predict = sgd_reg.predict(X_val_poly_scaled) # predict y validation value\n",
    "    train_errors.append(mean_squared_error(y_train, y_train_predict)) # get the training error and save array\n",
    "    val_errors.append(mean_squared_error(y_val, y_val_predict)) # get the validation error and save to array\n",
    "\n",
    "best_epoch = np.argmin(val_errors) # Returns the indices of the minimum values along the validation error array\n",
    "best_val_rmse = np.sqrt(val_errors[best_epoch]) # return the non-negative square-root of the best epoch\n",
    "# this value will be used to plot the point at which you have the best model performance\n",
    "\n",
    "# annotates the plot with an arrow to indicate the best performance on validation data.\n",
    "plt.annotate(f'Best model\\n at epoch = {best_epoch}',\n",
    "             xy=(best_epoch, best_val_rmse),\n",
    "             xytext=(best_epoch, best_val_rmse + 1),\n",
    "             ha=\"center\",\n",
    "             arrowprops=dict(facecolor='black', shrink=0.05),\n",
    "             fontsize=16,\n",
    "            )\n",
    "\n",
    "# this will plot a horizontal line to help improve the graph\n",
    "best_val_rmse -= 0.03  # just to make the graph look better\n",
    "plt.plot([0, n_epochs], [best_val_rmse, best_val_rmse], \"k:\", linewidth=2)\n",
    "\n",
    "# this will plot the validation and training set\n",
    "plt.plot(np.sqrt(val_errors), \"b-\", linewidth=3, label=\"Validation set\")\n",
    "plt.plot(np.sqrt(train_errors), \"r--\", linewidth=2, label=\"Training set\")\n",
    "\n",
    "# this will plot the legend, and axis labels\n",
    "plt.legend(loc=\"upper right\", fontsize=14)\n",
    "plt.xlabel(\"Epoch\", fontsize=14)\n",
    "plt.ylabel(\"RMSE\", fontsize=14)\n",
    "plt.savefig(\"images/earlystopping.png\", bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size:16px; border:1px solid black; padding:10px\">\n",
    "    <center><strong>Comments:</strong></center>\n",
    "<ul> \n",
    "    <li>Epoch between 0 - 250\n",
    "        <ul>\n",
    "            <li><strong>The models performance increases with as the number of Epochs increases</strong></li>\n",
    "            <li>The slope of both the Validation and Training curves are negative, and the error (RMSE) decreases for from 3 to less than 1 for Training, and [1-1.5] for the validation set.</li>\n",
    "            <li>Recall that the RMSE is the error between the actual value to the predicted value.</li>\n",
    "        </ul>\n",
    "    </li><br>\n",
    "    <li>Epoch between 200 - 300\n",
    "        <ul>\n",
    "            <li><font color=\"red\"><strong>The models performance for the validation set is optimal between this domain</strong></font></li> \n",
    "            <li>The error (RMSE) reaches its minimal between 200 - 250 epochs. The optimal model should be selected here. After 239 epochs, the error for the Validation Set beings to rise and the peformance degrades.</li>\n",
    "            <li>This means that the ability to 'predict' begins to decline for new data</li>\n",
    "            <li>The performance for the training set continue to also improve as the RMSE continues to drop.</li>\n",
    "            <li>This means that the model is starting to overfit and is learning 'everything' about the training data, and will not generalize well with new data.</li>\n",
    "        </ul>\n",
    "    </li><br>    \n",
    "    <li>Epoch between 300 - 500\n",
    "        <ul>\n",
    "            <li><strong>The models performance for the validation set is no longer optimal in this domain</strong></li> \n",
    "            <li>The performance of the validation data begins to decrease as the RMSE is increasing.</li>\n",
    "            <li>The performance for the training set continues to increase as teh RMSE is decreasing, this also means the model is overfitting.</li>\n",
    "            <li>The model no longer generalizes as well in this range.</li>            \n",
    "        </ul>\n",
    "    </li><br>    \n",
    "    <li>This learning curve serves as an example of how to identify the best model.</li><br>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border-top: 5px solid red;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size:16px; border:1px solid black; padding:10px\">\n",
    "    <center><h1>Final thoughts</h1></center>\n",
    "<ul>\n",
    "    <li>In this post, I discussed a method used to stop the training of a model early before it has overfit the training dataset and improve the generalization.</li><br>\n",
    "    <li>In this example I used Stochastic Gradient Descent, but the same process can be used for other common methods, such as neural networks.</li><br>\n",
    "    <li>Learning curves are great visualization techniques to monitor and and select the best model.</li><br> \n",
    "    <li>The use of early stopping requires the selection of a performance measure to monitor, a trigger for stopping training, and a selection of the model weights to use.</li><br>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border-top: 3px solid Black;\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
