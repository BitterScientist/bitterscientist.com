{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34901096",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
       "    return false;\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4da2ab4",
   "metadata": {},
   "source": [
    "<hr style=\"border-top: 5px solid black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5876b31d",
   "metadata": {},
   "source": [
    "<h1>Import Packages</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6dfcc9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a5c5ba",
   "metadata": {},
   "source": [
    "<hr style=\"border-top: 5px solid black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc20190f",
   "metadata": {},
   "source": [
    "<h1>Import Data</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab45ee82",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"authors_data.xlsx\"\n",
    "df = pd.read_excel(file, sheet_name=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2001d5c7",
   "metadata": {},
   "source": [
    "<hr style=\"border-top: 5px solid black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6731f34d",
   "metadata": {},
   "source": [
    "<h1>Inspect Data</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c103900",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9519a695",
   "metadata": {},
   "outputs": [],
   "source": [
    "authors = df.groupby(by=[\"ID\", \"Author\"])[\"Title\"].count().to_frame()\n",
    "authors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb42b066",
   "metadata": {},
   "source": [
    "<hr style=\"border-top: 5px solid black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228810c0",
   "metadata": {},
   "source": [
    "<h1>Word Clouds from Dataframe</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9deeb1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualize wordcloud\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "# Import Natural Language Toolkit\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8a1b52",
   "metadata": {},
   "source": [
    "<hr style=\"border-top: 2px solid black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d52337b",
   "metadata": {},
   "source": [
    "# Group All Titles by Author ID\n",
    "\n",
    " - Drop Duplicates\n",
    " - Group by and combine titles by author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0784e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "author_titles_df = df.drop_duplicates(subset=['Title'], keep='last')\n",
    "author_titles_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75d270d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "author_titles = author_titles_df.groupby(['ID', 'Author'], as_index = False).agg({'Title': ' '.join})\n",
    "author_titles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197e6b05",
   "metadata": {},
   "source": [
    "<hr style=\"border-top: 2px solid black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472b1344",
   "metadata": {},
   "source": [
    "# Word Tokenization\n",
    "\n",
    " - Action: Return a tokenized copy of string\n",
    " - word_tokenize(string)\n",
    " - Documentation: https://www.nltk.org/_modules/nltk/tokenize.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3e60f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of lists that contain tokens for each word in each list\n",
    "word_tokens = [word_tokenize(text) for text in author_titles.Title]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f488d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token processing, remove non alpha numeric tokens from each list\n",
    "cleaned_tokens = [[word for word in item if word.isalnum()] for item in word_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973aaeea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add results to dataframe\n",
    "author_titles[\"tokens\"] = cleaned_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ab9ac3",
   "metadata": {},
   "source": [
    "<hr style=\"border-top: 2px solid black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbcb7d28",
   "metadata": {},
   "source": [
    "# Word Token Lemmatizing\n",
    "\n",
    " - Action: Lemmatization is the process of converting a word to its base form.\n",
    " - WNlemmatizer = WordNetLemmatizer()\n",
    " - lemmanized = WNlemmatizer.lemmatize(token)\n",
    " - Documentation: https://www.nltk.org/_modules/nltk/stem/wordnet.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee703353",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemm_list = []\n",
    "WNlemmatizer = WordNetLemmatizer()\n",
    "for index, series in author_titles[\"tokens\"].iteritems():\n",
    "    lemm_list.append([WNlemmatizer.lemmatize(token).lower() for token in series])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a0b615",
   "metadata": {},
   "outputs": [],
   "source": [
    "author_titles[\"lemm_tokens\"] = lemm_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf47a17",
   "metadata": {},
   "source": [
    "<hr style=\"border-top: 2px solid black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077dcd34",
   "metadata": {},
   "source": [
    "# WordCloud Analysis\n",
    "\n",
    " - Reveals essential\n",
    " - Provides an overall sense of the text\n",
    " - Easy to grasp and engaging\n",
    " - wordcloud = WordCloud().generate(text)\n",
    " - Documentation: https://amueller.github.io/word_cloud/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671c5d0e",
   "metadata": {},
   "source": [
    "## Step 1: Filter Text Using Custom Stop Word List\n",
    "\n",
    " - Combine stop words from wordcloud, nltk, and custom list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75fc998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wordcloud stopwords\n",
    "wc_sw = STOPWORDS\n",
    "custom_sps = set(['made', 'nothing','able', 'given', 'wish', 'willing', 'wa', 'due', 'ha', 'did','etc', 'use', 'really', 'felt', 'personally', 'also', 'thing', 'well', 'little', 'got', 'one', 'lot', 'way', 'jus', 'sure'])\n",
    "cust_nltk_sw = set(stopwords.words('english')).union(custom_sps).union(wc_sw)\n",
    "print(f'The number of stop words in custom list is: {len(cust_nltk_sw)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b622229",
   "metadata": {},
   "source": [
    "## Step 2: Visualize WordCloud for all Title by Authors in dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176f6729",
   "metadata": {},
   "source": [
    "### Step 2a: Create function that converts tokenized text into a single string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f709527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will be used in the wordcloud\n",
    "def convert_text(series):\n",
    "    np_array = series.array\n",
    "    text = ' '.join(str(v) for v in np_array[0])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f529dd8d",
   "metadata": {},
   "source": [
    "### Step 2b: Create Wordcloud PNG files for each Author ID "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008d4f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "author_ids = author_titles.ID\n",
    "nbr_features = len(author_titles.ID)\n",
    "\n",
    "# for i in range(0,nbr_features):\n",
    "for id_num, num in zip(author_ids, range(0,nbr_features)):\n",
    "    fig = plt.figure(figsize=(30,15))\n",
    "    fig.subplots_adjust(hspace=1, wspace=0.2)\n",
    "\n",
    "    df0=author_titles[author_titles['ID']==id_num]['lemm_tokens']\n",
    "    text = convert_text(df0)\n",
    "    cloud = WordCloud(background_color=\"white\", max_words=10, stopwords=cust_nltk_sw, collocations=False).generate(text)\n",
    "\n",
    "    plt.imshow(cloud, interpolation='bilinear')\n",
    "    plt.title(id_num, fontsize = 20)\n",
    "    plt.axis('off')\n",
    "    plt.savefig(f\"{id_num}_wordcloud.png\", bbox_inches='tight', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86e39df",
   "metadata": {},
   "source": [
    "### Step 2c: Create figure that contains all wordclouds, each as a subplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe02123",
   "metadata": {},
   "outputs": [],
   "source": [
    "author_ids = author_titles.ID\n",
    "nbr_features = len(author_titles.ID)\n",
    "fig = plt.figure(figsize=(30,15))\n",
    "fig.subplots_adjust(hspace=1, wspace=0.2)\n",
    "\n",
    "for id_num, num in zip(author_ids, range(1,nbr_features)):\n",
    "    df0=author_titles[author_titles['ID']==id_num]['lemm_tokens']\n",
    "    text = convert_text(df0)\n",
    "    cloud = WordCloud(background_color=\"white\", max_words=10, stopwords=cust_nltk_sw, collocations=False).generate(text)\n",
    "    ax = fig.add_subplot(4,3, num)\n",
    "    ax.imshow(cloud, interpolation='bilinear')\n",
    "    ax.set_title(id_num)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.savefig(f\"all_wordcloud.png\", bbox_inches='tight', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fdc8ca",
   "metadata": {},
   "source": [
    "## Step 3: Visualize WordCloud for all Titles\n",
    "\n",
    " - Combine All Titles\n",
    " - Generate WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c98ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = ' '.join(author_titles_df[\"Title\"])\n",
    "title_list = []\n",
    "title_list.append(titles)\n",
    "title_df = pd.DataFrame.from_dict({'ID': 'ID-All', 'Title' : title_list})\n",
    "title_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd48d33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of lists that contain tokens for each word in each list\n",
    "word_tokens = [word_tokenize(text) for text in title_df.Title]\n",
    "\n",
    "# Token processing, remove non alpha numeric tokens from each list\n",
    "cleaned_tokens = [[word for word in item if word.isalnum()] for item in word_tokens]\n",
    "# Add results to dataframe\n",
    "title_df[\"tokens\"] = cleaned_tokens\n",
    "\n",
    "lemm_list = []\n",
    "WNlemmatizer = WordNetLemmatizer()\n",
    "for index, series in title_df[\"tokens\"].iteritems():\n",
    "    lemm_list.append([WNlemmatizer.lemmatize(token).lower() for token in series])\n",
    "\n",
    "title_df[\"lemm_tokens\"] = lemm_list\n",
    "\n",
    "\n",
    "# Wordcloud stopwords\n",
    "wc_sw = STOPWORDS\n",
    "custom_sps = set(['made', 'nothing','able', 'given', 'wish', 'willing', 'wa', 'due', 'ha', 'did','etc', 'use', 'really', 'felt', 'personally', 'also', 'thing', 'well', 'little', 'got', 'one', 'lot', 'way', 'jus', 'sure'])\n",
    "cust_nltk_sw = set(stopwords.words('english')).union(custom_sps).union(wc_sw)\n",
    "print(f'The number of stop words in custom list is: {len(cust_nltk_sw)}')\n",
    "\n",
    "title_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad85f897",
   "metadata": {},
   "outputs": [],
   "source": [
    "author_id = title_df.ID\n",
    "nbr_features = len(title_df.ID)\n",
    "\n",
    "fig = plt.figure(figsize=(30,15))\n",
    "\n",
    "df0=title_df['lemm_tokens']\n",
    "text = convert_text(df0)\n",
    "cloud = WordCloud(background_color=\"white\", max_words=10, stopwords=cust_nltk_sw, collocations=False).generate(text)\n",
    "\n",
    "plt.imshow(cloud, interpolation='bilinear')\n",
    "plt.title(title_df.ID.values, fontsize = 20)\n",
    "plt.axis('off')\n",
    "plt.savefig(f\"{author_id}_wordcloud.png\", bbox_inches='tight', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47389e9",
   "metadata": {},
   "source": [
    "<hr style=\"border-top: 5px solid black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834f8471",
   "metadata": {},
   "source": [
    "<h1>Create New DataFrame for Displaying WordCloud Images</h1>\n",
    "\n",
    " - Concatenate author_titles and titles\n",
    " - Add column for url\n",
    " - Add url\n",
    " - Export as CSV for tableau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c72557a",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [title_df, author_titles]\n",
    "word_cloud_df = pd.concat(frames, ignore_index=True)\n",
    "word_cloud_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc32388b",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_lists = ['https://i.imgur.com/NGDC7RA.png','https://i.imgur.com/HE8dkGa.png','https://i.imgur.com/EtQDWsW.png', 'https://i.imgur.com/bxm8T6x.png', 'https://i.imgur.com/2Gch9y0.png','https://imgur.com/YlKbGz8', 'https://i.imgur.com/UzCst7H.png']\n",
    "word_cloud_df[\"urls\"] = url_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19adf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_cloud_df.to_csv(\"id_wordclouds.csv\", index_label=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8451f8ce",
   "metadata": {},
   "source": [
    "<hr style=\"border-top: 5px solid black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c965f232",
   "metadata": {},
   "source": [
    "<h1>Network Analysis</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6768643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b77c285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a dataframe with your connections\n",
    "df = pd.DataFrame({ 'from':['A', 'B', 'C','A'], 'to':['D', 'A', 'E','C']})\n",
    " \n",
    "# Build your graph\n",
    "G=nx.from_pandas_edgelist(df, 'from', 'to')\n",
    " \n",
    "# Chart with Custom edges:\n",
    "nx.draw(G, with_labels=True, width=5, edge_color=\"skyblue\", style=\"solid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88671413",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"authors_data.xlsx\"\n",
    "df = pd.read_excel(file, sheet_name=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af6d9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd52ea0",
   "metadata": {},
   "source": [
    "## Prepare Data for Network Analysis\n",
    "\n",
    " - Subset for projects with co-contributors\n",
    " - Subset for only ID, Co-ID and Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbe52ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "network_df = df[df['Has_Contributors']==\"Yes\"][['ID', \"Co-ID\", \"Title\"]]\n",
    "network_df.rename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aef564a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build your graph\n",
    "fig = plt.figure(figsize=(30,15))\n",
    "G=nx.from_pandas_edgelist(network_df, 'Co-ID', 'ID')\n",
    " \n",
    "# Chart with Custom edges:\n",
    "nx.draw(G, with_labels=True, width=5, font_size=20, edge_color=\"skyblue\", style=\"solid\", alpha=0.75, font_weight=\"bold\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d4d14f",
   "metadata": {},
   "source": [
    "## Network that shows Authors Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b61bdc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "authors = list(network_df.ID.unique())\n",
    "co_contributors = list(network_df['Co-ID'].unique())\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "\n",
    "# 1. Create the graph\n",
    "g = nx.from_pandas_edgelist(network_df, source='Co-ID', target='ID') \n",
    "\n",
    "# 2. Create a layout for our nodes \n",
    "layout = nx.spring_layout(g,iterations=50)\n",
    "\n",
    "# 3. Draw the parts we want\n",
    "# Edges thin and grey\n",
    "# Co-Contributors small and grey\n",
    "# Authors sized according to their number of connections\n",
    "# Authors blue\n",
    "# Labels for Authors ONLY\n",
    "# People who are highly connected are a highlighted color\n",
    "\n",
    "# Go through every Authors name, ask the graph how many\n",
    "# connections it has. Multiply that by 80 to get the circle size\n",
    "author_size = [g.degree(author) * 80 for author in authors]\n",
    "nx.draw_networkx_nodes(g, \n",
    "                       layout, \n",
    "                       nodelist=authors, \n",
    "                       node_size=author_size, # a LIST of sizes, based on g.degree\n",
    "                       node_color='lightblue')\n",
    "\n",
    "# Draw EVERYONE\n",
    "nx.draw_networkx_nodes(g, layout, nodelist=co_contributors, node_color='#cccccc', node_size=100)\n",
    "\n",
    "# Draw POPULAR PEOPLE\n",
    "popular_people = [person for person in co_contributors if g.degree(person) > 1]\n",
    "nx.draw_networkx_nodes(g, layout, nodelist=popular_people, node_color='orange', node_size=100)\n",
    "\n",
    "nx.draw_networkx_edges(g, layout, width=1, edge_color=\"#cccccc\")\n",
    "\n",
    "node_labels = dict(zip(authors, authors))\n",
    "nx.draw_networkx_labels(g, layout, labels=node_labels)\n",
    "\n",
    "# 4. Turn off the axis because I know you don't want it\n",
    "plt.axis('off')\n",
    "\n",
    "plt.title(\"Collaboration Network Analysis\")\n",
    "\n",
    "# 6. Save Image\n",
    "plt.savefig(f\"network_AuthorsLabeled.png\", bbox_inches='tight', dpi=300)\n",
    "\n",
    "# 5. Tell matplotlib to show it\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c91b05a",
   "metadata": {},
   "source": [
    "## Network that shows Authors and Co-contributor IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d39e54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "authors = list(network_df.ID.unique())\n",
    "co_contributors = list(network_df['Co-ID'].unique())\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "\n",
    "# 1. Create the graph\n",
    "g = nx.from_pandas_edgelist(network_df, source='Co-ID', target='ID') \n",
    "\n",
    "# 2. Create a layout for our nodes \n",
    "layout = nx.spring_layout(g,iterations=50)\n",
    "\n",
    "# 3. Draw the parts we want\n",
    "# Edges thin and grey\n",
    "# Co-Contributors small and grey\n",
    "# Authors sized according to their number of connections\n",
    "# Authors blue\n",
    "# Labels for Authors ONLY\n",
    "# People who are highly connected are a highlighted color\n",
    "\n",
    "# Go through every Authors name, ask the graph how many\n",
    "# connections it has. Multiply that by 80 to get the circle size\n",
    "author_size = [g.degree(author) * 80 for author in authors]\n",
    "nx.draw_networkx_nodes(g, \n",
    "                       layout, \n",
    "                       nodelist=authors, \n",
    "                       node_size=author_size, # a LIST of sizes, based on g.degree\n",
    "                       node_color='lightblue')\n",
    "\n",
    "# Draw EVERYONE\n",
    "nx.draw_networkx_nodes(g, layout, nodelist=co_contributors, node_color='#cccccc', node_size=100)\n",
    "\n",
    "# Draw POPULAR PEOPLE\n",
    "popular_people = [person for person in co_contributors if g.degree(person) > 1]\n",
    "nx.draw_networkx_nodes(g, layout, nodelist=popular_people, node_color='orange', node_size=100)\n",
    "\n",
    "nx.draw_networkx_edges(g, layout, width=1, edge_color=\"#cccccc\")\n",
    "\n",
    "node_labels = dict(zip(authors, authors))\n",
    "nx.draw_networkx_labels(g, layout, labels=node_labels)\n",
    "nx.draw_networkx_labels(g, layout, labels=dict(zip(co_contributors,co_contributors)))\n",
    "\n",
    "# 4. Turn off the axis because I know you don't want it\n",
    "plt.axis('off')\n",
    "\n",
    "plt.title(\"Collaboration Network Analysis\")\n",
    "\n",
    "# 6. Save Image\n",
    "plt.savefig(f\"network_All_Labeled.png\", bbox_inches='tight', dpi=300)\n",
    "\n",
    "# 5. Tell matplotlib to show it\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9419484d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
