<!DOCTYPE html>
<html>

<head>
  <meta charset="UTF-8">
  <title>ML Performance Measures - ROC Curve</title>
  <!-- Bootstrap -->
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.4.1/jquery.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js"></script>


  <!-- Import mapping dependencies -->
  <link rel="stylesheet" href="https://unpkg.com/leaflet@1.4.0/dist/leaflet.css" />
  <script src="https://unpkg.com/leaflet@1.4.0/dist/leaflet.js"></script>
  <!-- Styling module -->
  <link rel="stylesheet" type="text/css" href='projects/la_weapon_crimes/static/css/map.css' )>

  <!-- Navbar -->
  <link rel="stylesheet" type="text/css" href='../../static/css/navbar.css' )>

  <!-- Footer -->
  <link rel="stylesheet" type="text/css" href='../../static/css/footer.css' )>

  <!-- General Themes -->
  <link rel="stylesheet" type="text/css" href='../../static/css/theme.css' )>
  <link rel="stylesheet" type="text/css" href='../../static/css/general_landing.css' )>
  <link rel="stylesheet" type="text/css" href='../../static/css/post-table.css' )>
  <link rel="stylesheet" type="text/css" href='../../static/css/code_blocks.css' )>
</head>


<body>
  <div id="page-container">
    <div id="content-wrap">
      <div w3-include-html="/folders/navbar_footer/navbar_pages.html"></div>


      <!-- all other page content -->
      <!-- Webpage title -->
      <div class="title">
        <h1>Making Sense of the ROC Curve.</h1>
      </div>

      <!-- Last updated -->
      <!-- Last updated -->
      <div class="date_updated">
        <p class="post-timestamp">First Posted: 3-9-2020<br>
          Last Updated: 3-9-2020</p>
      </div>

      <!-- Main container for the subfield -->

      <!-- Sets up bootstrap - this page only has one row -->
      <div class="row">

        <!-- The first column of this row is size 4 - the background panel -->
        <div class="col-sm-12">

          <!-- Side panel container for background information -->
          <div class="background-panel">
            <div class="caption">
              <p>Background Info</p>
            </div>
            <div class="background-info">
              <a href="projects/MNIST/static/images/roc_curve_example.png">
                <img src="projects/MNIST/static/images/roc_curve_example.png" alt="html" class="background-pic"
                  width="75%" height="10%"></a>
              <div class="background-img-caption">
                <p></p>
              </div>
              <p class="general-p">
                <B>The Receiver Operating Characteristic (ROC) curve</B>
                <ul>
                  <li>Is a tool used to evaluate the performance of binary classifiers.</li>
                  <li>It plots the recall (the true positive rate (TPR)) against the false positive rate (FPR).</li>
                  <li>The false positive rate is 1 - the ratio of negative instances that were correctly classified as
                    negative.</li>
                  <li>The true negative rate is also referred to as the sensitivity.</li>
                </ul>
              </p>
              <p class="general-p"><strong>Why is this important?</strong><br>
                This is a plot that gives insight into the errors being made by the classifier.
              </p>
              <p class="general-p"><strong>The project goal</strong><br>
                To demonstrate how to create an ROC curve, and explain how to interpret the plot.
              </p>
            </div>
            <div class="resources">
              <div class="sub-title">
                <h4>Resources & Citations</h4>
              </div>
              <ul class="resources-ul">
                <li><a href="https://en.wikipedia.org/wiki/MNIST_database" target="_blank">1.
                    MNIST database. Wiki.
                </li>
                <li><a href="https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/"
                    target="_blank">2. Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd
                    Edition, by Aurelien Geron (O'Rilley). 2019 Kiwisoft SAS, 978-1-492-03264-9</a>
                </li>
                <li><a href="http://yann.lecun.com/exdb/mnist/" target="_blank">3.
                    The MNIST database of handwritten digits with 784 features. Yann LeCun, Corinna Cortes,
                    Christopher JC Burges.
                </li>
              </ul>
            </div>
          </div>

          <div class="sub-section">
            <center>
              <a href="projects/MNIST/Classification_MNIST_ROC.ipynb" download>Download Jupyter Notebook File</a>
            </center>
            <div class="table_of_contents">
              <p class="content-list">
                <center>
                  <h4>Research Objectives</h4>
                </center>
                <ol>
                  <li><a href="#objective1">Brief Intro to the MNIST Dataset</a></li>
                  <li><a href="#objective2">Train and Measure SGDClassifier Model Accuracy</a></li>
                  <li><a href="#objective3">Preparing the ROC Curve</a></li>
                  <li><a href="#objective4">Create the ROC Curve</a></li>
                  <li><a href="#objective5">The Anatomy of a ROC Curve</a></li>
                  <li><a href="#objective6">Calculate the Area Under the ROC Curve</a></li>
                  <!-- <li><a href="#objective7"></a></li> -->
                </ol>
              </p>
            </div>
            <hr class="sec-div">
            <p class="sub-title">
              <h4>Methods</h4>
            </p>
            <div id="methods">
              <p class="general-p">
                <strong><em>Data</em></strong>
                <br>
                The MNIST (Modified National Institute of Standards and Technology) dataset contains 70,000
                handwritten digits that is used in training and in testing the performance of new machine learning
                algorithms (1).
              </p>
              <hr>
              <p class="general-p">
                <strong><em>Analysis</em><br></strong>
                The programming language Python was used in this project. The matplotlib libraries were used to
                visualize data. The numpy and sklearn libraries
                were used in the machine learning models. Specifically, a binary, stochastic gradient descent classifier
                was used to train an algorithm to detect a single digit. A confusion matrix was then used to evaluate
                the performance of this model.
              </p>
              <hr>
              <p class="general-p">
                <strong><em>Objective</em></strong><br>
                The goal of this post is to demonstrate how to prepare a ROC curve and how to interpret the results.
                <br>
              </p>
            </div>

            <hr class="sec-div">

            <div id="results">
              <p class="sub-title">
                <h4>Results:
                </h4>
              </p>

              <div id="objective1">
                <p class="general-p">
                  <strong><em>Brief Intro to MNIST Dataset</em></strong><br>
                  &emsp;&emsp;&emsp;&emsp;The MNIST dataset is used to help explain the confusion matrix. Here is a
                  description of the dataset:
                  <ul>
                    <li>Instances: 70,000</li>
                    <li>Each instance encodes a small image of a handwritten digit</li>
                    <li>Each image consists of 784 columns, each represents single pixel intensity from 0 (white) to 255
                      (black).</li>
                    <li>We can convert an instance into a 2d image by reshaping data into a 28 x 28 array.</li>
                    <li>Figure 1 is an example of 100 images generated taken from the dataset.</li>
                  </ul>
                </p>

                <!-- Figure -->
                <a href="projects/MNIST/static/images/fig1.png">
                  <img src="projects/MNIST/static/images/fig1.png" width="60%" height=100% class="figure"></img></a>
                <p class="figure_title"><strong>Figure 1.</strong> Digits from the MNIST dataset.
                </p>
                <hr>

              </div>

              <div id="objective2">
                <p class="general-p">
                  <strong><em>Train and Measure SGDClassifier Model Accuracy</em></strong><br>
                  &emsp;&emsp;&emsp;&emsp;A Stochastic Gradient Descent (SGD) classifier was used to build an algorithm
                  that would detect a single digit. The number 7 was used in this binary classification example (Figure
                  2). This digit is present in 10.4% of the training dataset. To determine the ratio of correct
                  predictions, the accuracy of this model was measured using cross-validation (cross_val_score) with a
                  K-fold of 3. The accuracy for each fold was above 93%, which is not surprising because 10% of the data
                  are in fact 7, and the 90% are not. In fact, this is confirmed when a similar test to classify non-7
                  instances using the same method was conducted. The accuracy for the cross-validation is above 89% for
                  each fold. <br><br>
                  <strong>These results confirm that accuracy alone is not an ideal classification
                    performance measure for skewed dataset (when some classes are more represented than
                    others)</strong><br>
                </p>

                <a href="projects/MNIST/static/images/fig2.png">
                  <img src="projects/MNIST/static/images/fig2.png" width="30%" height=100% class="figure"></img></a>
                <p class="figure_title"><strong>Figure 2.</strong> Example of the number 7 taken from the MNIST dataset.
                </p>
                <!-- END OF OBJ2 DIV -->
              </div>
              <hr>

              <div id="objective3">
                <p class="general-p">
                  <strong><em>Preparing the ROC Curve</em></strong><br>
                  You would proceed only after you train your binary classifier.
                  <ol>
                    <li><b>Step 1:</b> <br>
                      Obtain your set of predicted scores using
                      <code>sklearn.model_selection.cross_val_predict(estimator, X, y=None, ..., method='predict')</code><br>
                      <b>Documentation: </b><br>
                      <a
                        href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_predict.html">
                        sklearn.model_selection.cross_val_predict()</a><br><br>
                      <b>Example: </b><br>
                      <code>y_scores = cross_val_predict(sgd_clf, X_train, y_train_7, cv=3, method="decision_function")</code><br><br>
                      <b>Output: </b><br>
                      <code>array([-26444.17375986, -29232.36194438, -24011.8285088 , ...,
                        -24767.21328011, -18076.58386084, -12330.55080778])</code><br><br>
                      <B>Explanation: </B><br>
                      Each element (prediction) in the y_scores array is a number that indicates if the algorithm
                      classified the instance as either a 7 or not a 7. <br>
                      Number < 0 was classified as 'not a 7' <br>
                        Number > 0 was classified as a '7'<br><br>
                    <li><b>Step 2:</b><br>
                      Compute the true positive rate and the false positive rate for each threshold value using<br>
                      <code>sklearn.metrics.roc_curve(y_true, y_score, ...)</code><br>
                      <b>Documentation: </b><br>
                      <a
                        href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html?highlight=roc_curve">
                        sklearn.metrics.roc_curve()</a><br><br>
                      <b>Example: </b><br>
                      <code>fpr, tpr, thresholds = roc_curve(y_train_7, y_scores)</code><br><br>
                      <b>Output: </b><br>
                      <b>false positive rate (fpr): </b><code>array([0.00000000e+00, 1.59616919e-04, 7.18276137e-03, ...,
                            9.99840383e-01, 1.00000000e+00, 1.00000000e+00])</code><br><br>
                      <b>true positive rate (trp): </b><code>array([0.00000000e+00, 1.59616919e-04, 7.18276137e-03, ...,
                          9.99840383e-01, 1.00000000e+00, 1.00000000e+00])</code><br><br>
                      <b>thresholds: </b><code>array([  42743.31021631,   42742.31021631,   29831.60115424, ...,
                          -58240.45434254,  -58241.54714719, -163657.52830886])</code><br><br>
                      <B>False Positive (Fall-Out) Explained: </B><br>
                      Each element in the fpr (fall-out) array is the rate at which the model classified numbers as 7
                      when they were not 7. <br>
                      In the example illustrated in Figure 3, the image for 3 and image 1 were classified as 7, when they should not have
                      been.<br>
                      The fall-out rate in this example is 0.25<br><br>
                      <a href="projects/MNIST/static/images/roc_false_positive_rate.png">
                        <img src="projects/MNIST/static/images/roc_false_positive_rate.png" alt="html"
                          class="background-pic" width="75%" height="10%"></a><br>
                      <p class="figure_title"><strong>Figure 3.</strong> Example of a confusion matrix that explains the
                        False Positive (Fall-Out) Rate with 18 classifications.
                      </p>
                      <B>True Positive (Recall, Sensitivity) Explained: </B><br>
                      Each element in the trp array is the rate at which the model accurately classified all the images
                      at that point in the array.<br>
                      In the example illustrated in Figure 4, two image that should have been classified as 7 were classified as "not 7", when
                      they should not have been.<br>
                      The recall rate in this example is 0.8<br><br>
                      <a href="projects/MNIST/static/images/roc_true_positive_recall.png">
                        <img src="projects/MNIST/static/images/roc_true_positive_recall.png" alt="html"
                          class="background-pic" width="75%" height="10%"></a>
                      <p class="figure_title"><strong>Figure 4.</strong> Example of a confusion matrix that explains the
                        True Positive (Recall/Sensitivity) Rate with 18 classifications.
                      </p>
                  </ol>
                </p>
              </div>
              <hr>

              <div id="objective4">
                <p class="general-p">
                  <strong><em>Create the ROC Curve</em></strong><br>
                  <ol>
                    <li><b>Step 1:</b><br>
                      Create a function to plot ROC curve<br>
                      <code>def plot_roc_curve(fpr, tpr, label=None):<br>
                        &emsp;&emsp;&emsp;plt.plot(fpr, tpr, linewidth=2, label=label)<br>
                        &emsp;&emsp;&emsp;plt.plot([0, 1], [0, 1], 'k--')<br>
                        &emsp;&emsp;&emsp;plt.axis([0, 1, 0, 1])<br>                                
                        &emsp;&emsp;&emsp;plt.xlabel('False Positive Rate (Fall-Out)', fontsize=16)<br> 
                        &emsp;&emsp;&emsp;plt.ylabel('True Positive Rate (Recall)', fontsize=16)<br>
                        &emsp;&emsp;&emsp;plt.grid(True)</code><br>
                    </li><br>
                    <li><b>Step 2:</b><br>
                      Use matplotlib to render the plot with additional features<br>
                      <code>plt.figure(figsize=(8, 6))<br>
                        plot_roc_curve(fpr, tpr)<br>
                        plt.plot([20.837e-3, 20.837e-3], [0., 0.8901], "r:")<br>
                        plt.plot([0.0, 20.837e-3], [.8901, .8901], "r:")<br>
                        plt.show()<br>
                        </code><br>
                      In Figure 5 we see that the blue curve for this model approaches the top left corner.<br>
                      The red point indicates the the position where the recall rate for this model also has the maximum
                      precision score. <br> <br>
                      <a href="projects/MNIST/static/images/roc_curve.png">
                        <img src="projects/MNIST/static/images/roc_curve.png" alt="html" class="background-pic"
                          width="75%" height="10%"></a>
                          <p class="figure_title"><strong>Figure 5.</strong> ROC curve for the SGD classifier. The red circle highlights the chosen ratio where recall is 89%.
                          </p>
                    </li><br>
                  </ol>
                  </p>
              </div>
              <hr>
              <hr>
              <div id="objective5">
                <p class="general-p">
                  <strong><em>The Anatomy of a ROC Curve</em></strong><br>
                  The following elements are illustrated in Figure 6, and further explained in Figure 7. 
                  <ul>
                    <li><b>y-axis:</b> The true positive rate</li><br>
                    <li><b>x-axis:</b> The false positive rate aka specificity</li><br>
                    <li><b>points of curve:</b><br> Are obtained by plotting the recall and the specificity of each
                      point, as illustrated in figure().</li><br>
                    <li><b>Curves that indicate a good predictive model: </b><br>
                      Moves towards the upper left corner of plot = Good</li><br>
                    <li><b>Why?</b> <br>Because the model is capturing as many of the true positive values (images that
                      are really 7), while having fewer false positives (images that are not 7 but classified as 7).  <br>
                      The formulas used in these calculations are illustrated in figure 7.
                    </li><br>
                    <li><b>Curves that indicate a poor predictive model: </b><br>
                      Curves that approach or fall below the red dash line = Bad </li><br>
                    <li><b>Why?</b><br>
                      Because as the curve approaches the red dash indicates that the model is classifying less images
                      that are 7, while the number of false positives (images that are not 7) are increased. These two
                      factors pulls the curve towards the bottom right corner. This is bad.<br>
                      The formulas used in these calculations are illustrated in figure 7.</li><br>
                      <li><b>The True Positive Rate vs False Positive Rate Trade off:</b><br>
                        As the True Positive Rate (recall) increases, so does the number of false positives are included by the classifier.<br>
                        Example, as you move along the blue curve in Figure 7, you increase the recall, but you also move along the x-axis, and increase the false positives in the system.<br>
                        The formulas used in these calculations are illustrated in figure 7.</li>

                  </ul>
                  <a href="projects/MNIST/static/images/roc_curve_example.png">
                    <img src="projects/MNIST/static/images/roc_curve_example.png" alt="html" class="background-pic"
                      width="75%" height="10%"></a><br>
                      <p class="figure_title"><strong>Figure 6.</strong> Example Receiver Operating Characteristic space, where the y-axis represents the true positive rate and the x-axis represents the false positive rate for all possible thresholds. The red dashed diagonal line represents random guesses.
                      </p>

                  <a href="projects/MNIST/static/images/roc_tpr_vs_fpr_explained.png">
                    <img src="projects/MNIST/static/images/roc_tpr_vs_fpr_explained.png" alt="html"
                      class="background-pic" width="75%" height="10%"></a>
                      <p class="figure_title"><strong>Figure 7.</strong> Infographic explaning the anatomy of the ROC curve.
                      </p>
                </p>

              </div>
              <hr>

              <div id="objective6">
                <p class="general-p">
                  <strong><em>Calculate the Area Under the ROC Curve</em></strong><br>
                  <ul>
                    <li>
                      It is common to create models using several classifiers.
                    </li>
                    <li>
                      To determine which classifier has the best performance, the ROC curve and the Area Under the ROC
                      (AUC) are often used to make the comparison.
                    </li>
                    <li>
                      In this example, the RandomForestClassifier was used to prepare a new model to classify images as
                      either 7 or not 7.
                    </li>
                    <li>
                      In this example, the RandomForestClassifier not only had an ROC curve closer to the upper left
                      corner, but the AUC was also higher than the SGD AUC by 0.011.<br>
                      These two results suggest that the RandomForestClassifier is a better model.
                    </li>
                  </ul><br>
                  <ol>
                    <h4>How to calculate the Area Under the ROC Curve:</h4>
                    <li><b>Step 1: Import method</b><br>
                      <code>from sklearn.metrics import roc_auc_score</code></li><br>
                    <li><b>Step 2: Evoke roc_auc_score() function</b><br>
                      <code>roc_auc_score(y_train_7, y_scores)</code></li><br>
                    <li><b>Output for SGD ROC Curve: </b><br>
                      <code>0.987</code>
                    </li><br>
                    <li><b>Output for the Random Forest ROC Curve: </b><br>
                      <code>0.998</code>
                    </li>
                  </ol>
                </p>

                <a href="projects/MNIST/static/images/roc_curve_comparison_plot.png">
                  <img src="projects/MNIST/static/images/roc_curve_comparison_plot.png" alt="html"
                    class="background-pic" width="75%" height="10%"></a>
                    <p class="figure_title"><strong>Figure 8.</strong> Comparison of the ROC curves between the Random Forest classifier and the SGD classifier.
                    </p>
              </div>

              <hr>
              <div id="objective7">
                <p class="general-p">
                  <strong><em>Concluding Remarks</em></strong><br>
                  &emsp;&emsp;&emsp;&emsp;The ROC is an extremely useful tool with binary classifiers.  This tool enables you to evaluate the performance of your model, and should be added to your work flow. Unlike the the precision and recall curve (described here <a href="ds_confusion_matrix.html">Confusion Matrix</a>), the ROC plots the recall against the false positive rate. It also has a trade-off, the higher the recall, the more false positive the classifier produces.  A good classifier moves toward the top-left corner, and has an AUC that approaches 1.
                </p>

              </div>

            </div>
          </div>

          <!-- ################################ TEMPLATES ########################## -->


          <!-- Figure template -->
          <!-- <a href="projects/">
                <img src="projects/" width="100%" height=100% class="figure"></img></a>
              <p class="figure_title"><strong>Figure 1. </strong>
              </p> -->

          <!-- Table template -->
          <!-- <p class="table_title"><strong>Table 1. </strong> Table title<br>
              </p>
              <a href="projects/">
                <img src="projects/" width="75%" height=100% class="table_img"></img></a> -->


          <!-- CLOSING DIV FOR SUB-SECTION -->
        </div>


        <!-- ################################ DO NOT ENTER ########################## -->
        <div class="sub-entry">
          <div w3-include-html="ds_blogs_table.html"></div>
        </div>
      </div>
      <!-- Div for the bootstrap row -->
    </div>
    <!-- CLOSING MAIN -->
    </main>
    <!-- CLOSING DIV FOR CONTENT-WRAPPER -->
    <!--This div for footer-page div-->
    <div w3-include-html="/folders/navbar_footer/footer.html"></div>
    <!-- CLOSING DIV FOR PAGE-CONTAINER -->
  </div>

  <!-- THESE ARE KEY FOR THE D3JS -->
  <script src="https://d3js.org/d3.v5.min.js"></script>
  <!-- THIS IS TO ADD THE HEADER AND FOOTER -->
  <script type="text/javascript" src="../../static/js/include.js"></script>
  <!-- Mapping js -->
  <script type="text/javascript" src="projects/la_weapon_crimes/static/js/mapping.js"></script>
</body>

</html>