<!DOCTYPE html>
<html>

<head>
  <meta charset="UTF-8">
  <title>ML Performance Measures - Confusion Matrix</title>
  <!-- Bootstrap -->
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.4.1/jquery.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js"></script>


  <!-- Import mapping dependencies -->
  <link rel="stylesheet" href="https://unpkg.com/leaflet@1.4.0/dist/leaflet.css" />
  <script src="https://unpkg.com/leaflet@1.4.0/dist/leaflet.js"></script>
  <!-- Styling module -->
  <link rel="stylesheet" type="text/css" href='projects/la_weapon_crimes/static/css/map.css' )>

  <!-- Navbar -->
  <link rel="stylesheet" type="text/css" href='../../static/css/navbar.css' )>

  <!-- Footer -->
  <link rel="stylesheet" type="text/css" href='../../static/css/footer.css' )>

  <!-- General Themes -->
  <link rel="stylesheet" type="text/css" href='../../static/css/theme.css' )>
  <link rel="stylesheet" type="text/css" href='../../static/css/general_landing.css' )>
  <link rel="stylesheet" type="text/css" href='../../static/css/post-table.css' )>
  <link rel="stylesheet" type="text/css" href='../../static/css/code_blocks.css' )>
</head>


<body>
  <div id="page-container">
    <div id="content-wrap">
      <div w3-include-html="/folders/navbar_footer/navbar_pages.html"></div>


      <!-- all other page content -->
      <!-- Webpage title -->
      <div class="title">
        <h1>Making Sense of the Confusion Matrix.</h1>
      </div>

      <!-- Last updated -->
      <!-- Last updated -->
      <div class="date_updated">
        <p class="post-timestamp">First Posted: 2-29-2020<br>
          Last Updated: 2-30-2020</p>
      </div>

      <!-- Main container for the subfield -->

      <!-- Sets up bootstrap - this page only has one row -->
      <div class="row">

        <!-- The first column of this row is size 4 - the background panel -->
        <div class="col-sm-12">

          <!-- Side panel container for background information -->
          <div class="background-panel">
            <div class="caption">
              <p>Background Info</p>
            </div>
            <div class="background-info">
              <a href="projects/MNIST/static/images/conf_matrix.png">
                <img src="projects/MNIST/static/images/conf_matrix.png" alt="html" class="background-pic" width="75%"
                  height="10%"></a>
              <div class="background-img-caption">
                <p></p>
              </div>
              <p class="general-p">
                <B>Confusion Matrix (CM)</B> <br>
                <ul>
                  <li>Is a method used to evaluate the performance of a classifier by counting the times an instance
                    is either properly classified, or misclassified</li>
                  <li>The confusion matrix places instances in either: True Negative, False Positive, False Negative,
                    and True Positive categories.</li>
                  <li>The precision, recall, and F1 scores are calculated from values taken from the CM.</li>
                  <li>This method requires having the actual and predicted values.</li>
                  <li>CM is a better performance measure than accuracy.</li>
                </ul>

              </p>
              <p class="general-p"><strong>Why is this important?</strong><br>
                <ul>
                  <li>Provides insights into the errors being made by the classifier.</li>
                  <li>More informative measure than the accuracy alone.</li>
                </ul>
              </p>
              <p class="general-p"><strong>The project goal</strong><br>
                To help explain the confusion matrix.
              </p>
            </div>
            <div class="resources">
              <div class="sub-title">
                <h4>Resources & Citations</h4>
              </div>
              <ul class="resources-ul">
                <li><a href="https://en.wikipedia.org/wiki/MNIST_database" target="_blank">1.
                    MNIST database. Wiki.
                </li>
                <li><a href="https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/"
                    target="_blank">2. Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd
                    Edition, by Aurelien Geron (O'Rilley). 2019 Kiwisoft SAS, 978-1-492-03264-9</a>
                </li>
                <li><a href="http://yann.lecun.com/exdb/mnist/" target="_blank">3.
                    The MNIST database of handwritten digits with 784 features. Yann LeCun, Corinna Cortes,
                    Christopher JC Burges.
                </li>
              </ul>
            </div>
          </div>

          <div class="sub-section">
            <center>
              <a href="projects/MNIST/Classification_MNIST.ipynb" download>Download Jupyter Notebook File</a>
            </center>
            <div class="table_of_contents">
              <p class="content-list">
                <center>
                  <h4>Research Objectives</h4>
                </center>
                <ol>
                  <li><a href="#objective1">Brief Intro to the MNIST Dataset</a></li>
                  <li><a href="#objective2">Train and Measure SGDClassifier Model Accuracy</a></li>
                  <li><a href="#objective3">Confusion Matrix</a></li>
                  <li><a href="#objective4">Precision Explained!</a></li>
                  <li><a href="#objective5">Recall Explained!</a></li>
                  <li><a href="#objective6">F1 Explained!</a></li>
                  <li><a href="#objective7">The Precision and Recall Trade Offs</a></li>
                </ol>
              </p>
            </div>
            <hr class="sec-div">
            <p class="sub-title">
              <h4>Methods</h4>
            </p>
            <div id="methods">
              <p class="general-p">
                <strong><em>Data</em></strong>
                <br>
                The MNIST (Modified National Institute of Standards and Technology) dataset contains 70,000
                handwritten digits that is used in training and in testing the performance of new machine learning
                algorithms (1).
              </p>
              <hr>
              <p class="general-p">
                <strong><em>Analysis</em><br></strong>
                The programming language Python was used in this project. The matplotlib libraries were used to
                visualize data. The numpy and sklearn libraries
                were used in the machine learning models. Specifically, a binary, stochastic gradient descent classifier
                was used to train an algorithm to detect a single digit. A confusion matrix was then used to evaluate
                the performance of this model.
              </p>
              <hr>
              <p class="general-p">
                <strong><em>Objective</em></strong><br>
                The goal of this post is to explain the confusion matrix, and more specifically, accuracy, precision,
                recall, and F1.
                <br>
              </p>
            </div>

            <hr class="sec-div">

            <div id="results">
              <p class="sub-title">
                <h4>Results:
                </h4>
              </p>

              <div id="objective1">
                <p class="general-p">
                  <strong><em>Brief Intro to MNIST Dataset</em></strong><br>
                  &emsp;&emsp;&emsp;&emsp;The MNIST dataset is used to help explain the confusion matrix. Here is a
                  description of the dataset:
                  <ul>
                    <li>Instances: 70,000</li>
                    <li>Each instance encodes a small image of a handwritten digit</li>
                    <li>Each image consists of 784 columns, each represents single pixel intensity from 0 (white) to 255
                      (black).</li>
                    <li>We can convert an instance into a 2d image by reshaping data into a 28 x 28 array.</li>
                    <li>Figure 1 is an example of 100 images generated taken from the dataset.</li>
                  </ul>
                </p>

                <!-- Figure -->
                <a href="projects/MNIST/static/images/fig1.png">
                  <img src="projects/MNIST/static/images/fig1.png" width="60%" height=100% class="figure"></img></a>
                <p class="figure_title"><strong>Figure 1.</strong> Digits from the MNIST dataset.
                </p>
                <hr>

              </div>

              <div id="objective2">
                <p class="general-p">
                  <strong><em>Train and Measure SGDClassifier Model Accuracy</em></strong><br>
                  &emsp;&emsp;&emsp;&emsp;A Stochastic Gradient Descent (SGD) classifier was used to build an algorithm
                  that would detect a single digit. The number 7 was used in this binary classification example (Figure
                  2). This digit is present in 10.4% of the training dataset. To determine the ratio of correct
                  predictions, the accuracy of this model was measured using cross-validation (cross_val_score) with a
                  K-fold of 3. The accuracy for each fold was above 93%, which is not surprising because 10% of the data
                  are in fact 7, and the 90% are not. In fact, this is confirmed when a similar test to classify non-7
                  instances using the same methods is conducted. The accuracy for the cross-validation is above 89% for
                  each fold. <strong>These results confirm that accuracy alone is not an ideal classification
                    performance measure for skewed dataset (when some classes are more represented than
                    others)</strong><br>
                </p>

                <a href="projects/MNIST/static/images/fig2.png">
                  <img src="projects/MNIST/static/images/fig2.png" width="30%" height=100% class="figure"></img></a>
                <p class="figure_title"><strong>Figure 2.</strong> Example of the number 7 taken from the MNIST dataset.
                </p>
                <!-- END OF OBJ2 DIV -->
              </div>
              <hr>

              <div id="objective3">
                <p class="general-p">
                  <strong><em>Confusion Matrix</em></strong><br>
                  A confusion matrix requires two sets of data:<br>
                  1) The actual labels for the dataset (if its a 7 or not a 7)<br>
                  2) The predicted labels for the dataset that were generated from the classifier<br>
                  These two arrays are then used to determine the true negative, false positive, false negative, and
                  true positive classifications, and is represented in the 2D plot in figure 3.<br>
                  <ul><strong>Row and Column Overview</strong>
                    <li>The first row contains all the actual negative instances in the dataset. In this example, every
                      instance that is not 7</li>
                    <li>The second row contains all the actual positive instances in the training dataset. All the
                      instances that were 7.</li>
                    <li>The first column contains all the data that the algorithm predicted were negative (NOT 7).</li>
                    <li>The second column contains all the data that the algorithm classified as positive (7).</li>
                  </ul>
                  <ul><strong>Overview of each cell in 2D plot</strong><br>
                    Figure 4 summarizes the following:
                    <li>a) True Negative: contains all the data that the algorithm classified as negative (NOT 7), and
                      were NOT 7 in the actual dataset. So the classifier got it right!</li>
                    <li>b) False Positive. contains all the data that the algorithm classified as positive (7), but they
                      are NOT 7 in the actual dataset, they are really another number. It got it wrong!</li>
                    <li>c) False Negative: contains all the data that the algorithm classified as negative (NOT 7), but
                      were in fact 7 in the actual dataset.</li>
                    <li>d) True Positive: contains all the data that the algorithm classified as positive (7) and were 7
                      in the actual dataset.</li>
                  </ul>
                </p>
                <p class="general-p">
                  Taking into account the explanation above, we see that for the confusion matrix in this dataset in
                  figure 5:
                  <ul>
                    <li>a) The classifier accurately classified 98% of the actual negative (not 7) instances in the
                      dataset</li>
                    <li>b) The classifier erroneously classified 2% of the instances as 7 (false positives), when they
                      were not 7 in the actual dataset.</li>
                    <li>c) The classifier erroneously classified 11% of the data as NOT 7 (false negatives), when in
                      fact, these instances were 7 in the dataset. </li>
                    <li>d) The classifier accurately classified 89% of the instances as 7 (true positive), when they
                      were 7 in the actual dataset.</li>
                  </ul>
                  Taken together, these results suggest that the model was able to accurately classify non-7 digits, but
                  was only able to accurately classify real 7s 89% of the time. Figure 6 provides an example of a
                  confusion matrix if the model perfectly classified each instance. Notice that only cells a and d have
                  values, whereas cells b, c are 0.
                </p>

                <a href="projects/MNIST/static/images/conf_matrix.png">
                  <img src="projects/MNIST/static/images/conf_matrix.png" width="50%" height=100%
                    class="figure"></img></a>
                <p class="figure_title"><strong>Figure 3.</strong> Confusion Matrix model.
                </p>

                <a href="projects/MNIST/static/images/confusion_matrix2.png">
                  <img src="projects/MNIST/static/images/confusion_matrix2.png" width="50%" height=100%
                    class="figure"></img></a>
                <p class="figure_title"><strong>Figure 4.</strong> Confusion Matrix model.
                </p>

                <a href="projects/MNIST/static/images/fig3.png">
                  <img src="projects/MNIST/static/images/fig3.png" width="50%" height=100% class="figure"></img></a>
                <p class="figure_title"><strong>Figure 5.</strong> Confusion Matrix for SGD Classification of the digit
                  7 in training dataset.
                </p>

                <a href="projects/MNIST/static/images/fig4.png">
                  <img src="projects/MNIST/static/images/fig4.png" width="50%" height=100% class="figure"></img></a>
                <p class="figure_title"><strong>Figure 6.</strong> Example of a Confusion Matrix for SGD Classification
                  of
                  the digit 7 if algorithm perfectly classified each instance.</p>
              </div>
              <hr>

              <hr>
              <div id="objective4">
                <p class="general-p">
                  <strong><em>Precision: tell you how many of the <font color="red">predicted</font> "positives" were real.</em></strong><br>
                  <a href="projects/MNIST/static/images/precision.png">
                    <img src="projects/MNIST/static/images/precision.png" width="25%" height=100%
                      class="figure"></img></a>
                  Precision is a concise metric that is used to evaluate how well the model was at
                  <strong>classifying</strong> real positive value in the dataset. <br>
                  In this example, it indicates how many of the "Positives" were actually 7.<br>
                  Are the true positives greater than the false positives?<br>
                  Looking at the formula for precision, we see that as the false positives go down and approach 0, the
                  precision approaches 1 (which is perfection).<br>
                  The precision score in this example was 83%. This tells us that out of 10 instances that the algorithm
                  classified as 7, two of them were not 7, and were false positives.
                </p>
              </div>
              <hr>

              <div id="objective5">
                <p class="general-p">
                  <strong><em>Recall: tells you how many of the <font color="red">real</font> "positives" were captured.</em></strong><br>
                  <a href="projects/MNIST/static/images/recall.png">
                    <img src="projects/MNIST/static/images/recall.png" width="25%" height=100% class="figure"></img></a>
                  Recall is a concise metric that is used to evaluate how well the model was at
                  <strong>capturing</strong> all the real positive values in the dataset. <br>
                  - In this example, it indicates how many of the "Positives" were really 7.<br>
                  - Ask yourself, is the number of false negatives small?<br>
                  - As the number of predicted false negatives approach zero, the recall approaches 1.<br>
                  - The recall in this example was 86%. This tells us that:<br>
                  -- out of 10 predictions:<br>
                  ---- 8 predictions were correctly classified as 7<br>
                  ---- 2 predictions were misclassified as NOT a 7, and some other number<br>
                </p>
              </div>
              <hr>
              <hr>

              <div id="objective6">
                <p class="general-p">
                  <strong><em>F1 Explained!</em></strong><br>
                  <a href="projects/MNIST/static/images/F1_score.png">
                    <img src="projects/MNIST/static/images/F1_score.png" width="25%" height=100%
                      class="figure"></img></a>
                  The F1 score is the harmonic mean of the precision and recall.<br>
                  Precision and recall are combined in the F1 score to provide a single metric that is used when compare
                  two classifiers.<br>
                  In this example, the F1 score was 0.86.
                </p>
              </div>

              <hr>
              <div id="objective7">
                <p class="general-p">
                  <strong><em>The Precision and Recall Trade Offs</em></strong><br>
                  &emsp;&emsp;&emsp;&emsp;Precision and recall are powerful metrics, although there is a trade-off, if
                  you wish to increase the precision of a model, you will reduce recall, and vice versa.<br>
                  Why?
                  <ul>
                    <li>The SGDClassifier uses a decision function to classify an instance as either positive or
                      negative (in this binary classification example, 7 or not 7).</li>
                    <li>A threshold is used to facilitate the classification</li>
                    <li>If the score for an instance is above the threshold, then the instance is positive, and if it is
                      below, negative.</li>
                  </ul>
                </p>
                <p class="general-p">
                  The sklearn.metric precision_recall_curve() method was used to calculate the precision, recall, and
                  thresholds values for this dataset.  The results are plotted in Figure 7. We can see in this plot that if
                  we wanted the model to have a 90% precision (the percent of predicted positive classifications that are actually 7), the threshold would be ~2288.  At this threshold, the recall (the percent of the actual 7s that were captured in the predicted model) would be
                  around 83%. If we increased the precision, the threshold would increase, but the recall would
                  decrease. This means that the model would capture less of the real 7s, but it would classify less false positives, and be more precise in classifying real 7s.<br>
                  One way to find a good precision/recall trade-off is to plot the precision directly against the recall.  For this example, we see in Figure 8 that a precision of 86% would give us the optimal recall at 83%.
                </p>
                <a href="projects/MNIST/static/images/fig5.png">
                  <img src="projects/MNIST/static/images/fig5.png" width="50%" height=100% class="figure"></img></a>
                <p class="figure_title"><strong>Figure 7.</strong> The decision threshold for the precision and recall
                  plot.</p>

                <a href="projects/MNIST/static/images/fig6.png">
                  <img src="projects/MNIST/static/images/fig6.png" width="50%" height=100% class="figure"></img></a>
                <p class="figure_title"><strong>Figure 8.</strong> Precision versus recall plot.</p>
              </div>
            </div>
          </div>

          <!-- ################################ TEMPLATES ########################## -->


          <!-- Figure template -->
          <!-- <a href="projects/">
                <img src="projects/" width="100%" height=100% class="figure"></img></a>
              <p class="figure_title"><strong>Figure 1. </strong>
              </p> -->

          <!-- Table template -->
          <!-- <p class="table_title"><strong>Table 1. </strong> Table title<br>
              </p>
              <a href="projects/">
                <img src="projects/" width="75%" height=100% class="table_img"></img></a> -->


          <!-- CLOSING DIV FOR SUB-SECTION -->
        </div>


        <!-- ################################ DO NOT ENTER ########################## -->
        <div class="sub-entry">
          <div w3-include-html="ds_blogs_table.html"></div>
        </div>
      </div>
      <!-- Div for the bootstrap row -->
    </div>
    <!-- CLOSING MAIN -->
    </main>
    <!-- CLOSING DIV FOR CONTENT-WRAPPER -->
    <!--This div for footer-page div-->
    <div w3-include-html="/folders/navbar_footer/footer.html"></div>
    <!-- CLOSING DIV FOR PAGE-CONTAINER -->
  </div>

  <!-- THESE ARE KEY FOR THE D3JS -->
  <script src="https://d3js.org/d3.v5.min.js"></script>
  <!-- THIS IS TO ADD THE HEADER AND FOOTER -->
  <script type="text/javascript" src="../../static/js/include.js"></script>
  <!-- Mapping js -->
  <script type="text/javascript" src="projects/la_weapon_crimes/static/js/mapping.js"></script>
</body>

</html>